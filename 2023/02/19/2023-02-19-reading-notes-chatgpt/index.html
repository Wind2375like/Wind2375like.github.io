

<!DOCTYPE html>
<html lang="en" >



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Wind_like">
  <meta name="keywords" content="">
  
    <meta name="description" content="Reading notes of Yao’s notes of How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources (notion.site).">
<meta property="og:type" content="article">
<meta property="og:title" content="Reading Notes of How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources">
<meta property="og:url" content="http://example.com/2023/02/19/2023-02-19-reading-notes-chatgpt/index.html">
<meta property="og:site_name" content="Wind_like">
<meta property="og:description" content="Reading notes of Yao’s notes of How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources (notion.site).">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimage-20230218225348976.png">
<meta property="article:published_time" content="2023-02-19T11:50:37.000Z">
<meta property="article:modified_time" content="2023-02-19T11:54:08.551Z">
<meta property="article:author" content="Wind_like">
<meta property="article:tag" content="Reading Notes">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="ChatGPT">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimage-20230218225348976.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>Reading Notes of How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources - Wind_like</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  



  
<link rel="stylesheet" href="/css/heimu.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.4","typing":{"enable":true,"typeSpeed":70,"cursorChar":"|","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":true}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Wind_like</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Reading Notes of How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-02-19 12:50" pubdate>
          Sunday, February 19th 2023, 12:50 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          5.4k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          45 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Reading Notes of How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources</h1>
            
            
              <div class="markdown-body">
                
                <p>Reading notes of Yao’s notes of <a target="_blank" rel="noopener" href="https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1">How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources (notion.site)</a>.</p>
<span id="more"></span>
<blockquote>
<p>Keywords: code and instruction tuning; trade of between in-context learning and instruction tuning; unlock or inject the abilities; instruction-tuning, supervised instruction-tuning, and RLHF instruction-tuning; knowledge and reasoning.</p>
</blockquote>
<h2 id="Reference">Reference</h2>
<blockquote>
<p>Fu, Yao; Peng, Hao and Khot, Tushar. (Dec 2022). How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources. Yao Fu’s Notion. <a target="_blank" rel="noopener" href="https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1">https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1</a></p>
</blockquote>
<h2 id="Road-Map">Road Map</h2>
<img src="https://raw.githubusercontent.com/Wind2375like/I-m_Ghost/main/imgimage-20230218225348976.png" srcset="/img/loading.gif" lazyload alt="image-20230218225348976"  />
<p>Note: The base model for code-davinci-002 is highly likely not be the initial GPT-3 davinci model.</p>
<h2 id="Summary">Summary</h2>
<p>The authors’ conclusion:</p>
<ul>
<li>The <strong>language generation ability</strong> + <strong>basic world knowledge</strong> + <strong>in-context learning</strong> are from pretraining (<code>davinci</code>)</li>
<li>The ability to <strong>store a large amount of ==knowledge==</strong> is from the 175B scale.</li>
<li>The ability to <strong>follow instructions</strong> and <strong>generalizing to new tasks</strong> are from scaling instruction tuning (<code>davinci-instruct-beta</code>)</li>
<li>The ability to perform <strong>complex ==reasoning==</strong> is likely to be from training on code (<code>code-davinci-002</code>)</li>
<li>The ability to <strong>generate neutral, objective, safe, and informative answers</strong> are from alignment with human. Specifically:
<ul>
<li>If supervised tuning, the resulting model is <code>text-davinci-002</code></li>
<li>If RLHF, the resulting model is <code>text-davinci-003</code></li>
<li>Either supervised or RLHF, the models cannot outperform <code>code-davinci-002</code> on many tasks, which is called the <em>alignment tax</em>. (RLHF on 003 just recovers the in-context learning ability.)</li>
</ul>
</li>
<li>The <strong>dialog ability</strong> is also from RLHF (<code>ChatGPT</code>), specifically it tradeoffs in-context learning for:
<ul>
<li>Modeling dialog history</li>
<li>Increased informativeness</li>
<li>Rejecting questions outside the model’s knowledge scope</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>Ability</th>
<th>OpenAI Model</th>
<th>Training Method</th>
<th>OpenAI API</th>
<th>OpenAI Paper</th>
<th>Open Source Approximate</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3 Series</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Generation  <br>+ World Knowledge  <br>+ In-context Learning</td>
<td>GPT-3 Initial<br><br><strong>Many abilities already within this model, although superficially weak</strong></td>
<td>Language Modeling</td>
<td>Davinci</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.14165">GPT 3 Paper</a></td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.01068">Meta OPT</a><br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.05100">BLOOM</a><br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.09085">Galactica</a></td>
</tr>
<tr>
<td>+ Follow Human Instruction<br>+ Generalize to unseen task (free lunch of scaling instructions)</td>
<td>Instruct-GPT initial</td>
<td>Instruction Tuning</td>
<td>Davinci-Instruct-Beta</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.02155">Instruct-GPT paper</a></td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2110.08207">T0 paper</a><br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2109.01652">Google FLAN paper</a><br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.11416">Google FLAN paper</a></td>
</tr>
<tr>
<td>+ Code Understanding<br>+ Code Generation</td>
<td>Codex initial</td>
<td>Training on Code</td>
<td>Code-Cushman-001</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.03374">Codex Paper</a></td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.13474">Salesforce CodeGen</a></td>
</tr>
<tr>
<td>GPT-3.5 Series</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>++ Code Understanding<br>++ Code Generation<br>++ Complex Reasoning / Chain of Thought (<em>why</em>?)<br>+ Long-term dependency  (<em>probably</em>)</td>
<td>Current Codex<br><br><strong>Strongest model in GPT3.5 Series</strong></td>
<td>Training on text + code <br>Tuning on instructions</td>
<td>Code-Davinci-002</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.03374">Codex Paper</a></td>
<td>??</td>
</tr>
<tr>
<td>++ Follow Human Instruction<br>- In-context learning<br>- Reasoning<br>++ Zero-shot generation</td>
<td>Instruct-GPT supervised<br><br><strong>Trade in-context learning for zero-shot generation</strong></td>
<td>Supervised instruction tuning</td>
<td>Text-Davinci-002</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.02155">Instruct-GPT paper</a>, supervised part</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2110.08207">T0 paper</a><br/><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2109.01652">Google FLAN paper</a><br/><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.11416">Google FLAN paper</a></td>
</tr>
<tr>
<td>+ Follow human value<br>+ More detailed generation<br>+ In-context learning<br>+ Zero-shot generation</td>
<td>Instruct-GPT RLHF<br><br><strong>More aligned than 002, less performance loss</strong></td>
<td>Instruction tuning w. RLHF</td>
<td>Text-Davinci-003</td>
<td>Instruct-GPT paper, RLHF part <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2009.01325">Summarization .w human feedback</a></td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2209.14375">DeepMind Sparrow paper</a><br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.01241">AI2 RL4LMs</a></td>
</tr>
<tr>
<td>++ Follow human value<br>++ More detailed generation<br>++ Reject questions beyond its knowledge (why?)<br>++ Model dialog context<br>– In-context learning</td>
<td>ChatGPT<br><br><strong>Trade in-context learning for dialog history modeling</strong></td>
<td>Tuning on dialog w. RLHF</td>
<td>-</td>
<td>-</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2209.14375">DeepMind Sparrow paper</a><br/><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.01241">AI2 RL4LMs</a></td>
</tr>
</tbody>
</table>
<h2 id="Limitations">Limitations</h2>
<ol>
<li><strong>On-the-fly overwriting the model’s belief</strong>: when the model expresses its belief in something, it might be hard to correct it when the belief is wrong. There seems to be a hierarchy of how strong the belief is, which means that there exists a list of very strong core belief. ==<strong>It is extremely important to ensure such core belief should be absolutely 100% aligned with human.</strong>==</li>
<li><strong>Formal reasoning</strong>: the GPT-3.5 series cannot do reasoning within formal, strict systems like math or first-order logic.
<ol>
<li>The word “reasoning” is less well-defined. Yet if we view there is a spectrum of ambiguity like (a) very ambiguous, no reasoning; (b) mixture of logic and ambiguous statements; ©. no ambiguity has to be very rigorous, then,</li>
<li>The model can do very well on type (b) reasoning with ambiguity.</li>
<li>The model cannot do type © reasoning, <strong>yet whether such reasoning should be done by a language model or a symbolic system is up for discussion.</strong></li>
<li>Update: <a target="_blank" rel="noopener" href="https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/">Wolfram|Alpha as the Way to Bring Computational Knowledge Superpowers to ChatGPT—Stephen Wolfram Writings</a></li>
</ol>
</li>
<li><strong>Retrieval from the Internet</strong>: the GPT-3.5 series cannot directly search the internet (for now)
<ol>
<li>Likely tested within OpenAI: <a target="_blank" rel="noopener" href="https://openai.com/blog/webgpt/">WebGPT: Improving the Factual Accuracy of Language Models through Web Browsing (openai.com)</a></li>
<li><strong>Seperate the knowledge and reasoning</strong>: it would be ideal if we could offload the knowledge part to the outside retrieval system and let the language model only focus on reasoning.</li>
<li><strong>Combining LLMs (reasoning) and search (knowledge) is a good direction</strong>: LLMs are good for reasoning, not for knowledge. The knowledge within LLMs are unreliable and cannot be verified. On the other hand, the knowledge from search engine is orders or magnitude larger than LLM’s internal knowledge, can one can easily verify credibility of search results by checking the source.
<ol>
<li>Retrieval-augmented models?</li>
<li>Reduce the model size? (175B storing unreliable knowledge -&gt; search engines)</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="Questions-to-be-answered">Questions to be answered</h2>
<ol>
<li>How does the GPT-3.5 acquire the reasoning ability (CoT)? (Likely because of code training according to the authors).</li>
<li>Which kinds of abilities are unlocked/injected by what means<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote">&lt;span class=“hint–top hint–rounded” aria-label=““by what means” is hypothesized in the <a href="##summary">summary</a> section but some of them are not proved.”&gt;[1]</span></a></sup>? The authors’ hypothesis (according to scale):
<ol>
<li>Abilities from code training: injection</li>
<li>Abilities from instruction tuning: unlock</li>
</ol>
</li>
</ol>
<h2 id="What-I-need-to-further-know-about">What I need to further know about</h2>
<ul>
<li>instruction-tuning? supervised instruction-tuning? RLHF instruction-tuning?</li>
<li>code training?</li>
</ul>
<section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>“by what means” is hypothesized in the <a href="##summary">summary</a> section but some of them are not proved.
<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>
                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Reading-Notes/" class="category-chain-item">Reading Notes</a>
  
  
    <span>></span>
    
  <a href="/categories/Reading-Notes/NLP/" class="category-chain-item">NLP</a>
  
  
    <span>></span>
    
  <a href="/categories/Reading-Notes/NLP/ChatGPT/" class="category-chain-item">ChatGPT</a>
  
  

  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Reading-Notes/">#Reading Notes</a>
      
        <a href="/tags/NLP/">#NLP</a>
      
        <a href="/tags/ChatGPT/">#ChatGPT</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Reading Notes of How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources</div>
      <div>http://example.com/2023/02/19/2023-02-19-reading-notes-chatgpt/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Wind_like</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>February 19, 2023</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/02/12/2023-02-12-huggingface-parallel-training-solving-oom/" title="Huggingface parallel training for solving the CUDA out of memory issue">
                        <span class="hidden-mobile">Huggingface parallel training for solving the CUDA out of memory issue</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div class="disqus" style="width:100%">
    <div id="disqus_thread"></div>
    
      <script type="text/javascript">
        var disqus_config = function() {
          this.page.url = 'http://example.com/2023/02/19/2023-02-19-reading-notes-chatgpt/';
          this.page.identifier = '/2023/02/19/2023-02-19-reading-notes-chatgpt/';
        };
        Fluid.utils.loadComments('#disqus_thread', function() {
          var d = document, s = d.createElement('script');
          s.src = '//' + 'fluid' + '.disqus.com/embed.js';
          s.setAttribute('data-timestamp', new Date());
          (d.head || d.body).appendChild(s);
        });
      </script>
    
    <noscript>Please enable JavaScript to view the comments</noscript>
  </div>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  



  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});

    Fluid.events.registerRefreshCallback(function() {
      if ('mermaid' in window) {
        mermaid.init();
      }
    });
  });
</script>






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
      <div class="col-lg-7 mx-auto nopadding-x-md">
        <div class="container custom mx-auto">
          <link rel="stylesheet" href="https://npm.elemecdn.com/font-awesome/css/font-awesome.min.css"/>
        </div>
      </div>
    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> <br>竹杖芒鞋轻胜马 谁怕 一蓑烟雨任平生 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        Number of visits 
        <span id="busuanzi_value_site_pv"></span>
        
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        Number of visitors 
        <span id="busuanzi_value_site_uv"></span>
        
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  
<script src="https://cdn.jsdelivr.net/npm/i18n-js@3.8.0/app/assets/javascripts/i18n.min.js"></script>
<script src="/js/autoload.js"></script>
<script src="https://npm.elemecdn.com/jquery/dist/jquery.min.js"></script>



<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
